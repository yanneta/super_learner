{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Classification Datasets for Super Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GAMETES_Epistasis_2_Way_1000atts_0.4H_EDM_1_EDM_1_1', 'GAMETES_Epistasis_2_Way_20atts_0.1H_EDM_1_1', 'GAMETES_Epistasis_2_Way_20atts_0.4H_EDM_1_1', 'GAMETES_Epistasis_3_Way_20atts_0.2H_EDM_1_1', 'GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_50_EDM_2_001', 'GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_75_EDM_2_001', 'Hill_Valley_with_noise', 'Hill_Valley_without_noise', 'adult', 'agaricus_lepiota', 'allbp', 'allhyper', 'allhypo', 'allrep', 'analcatdata_aids', 'analcatdata_asbestos', 'analcatdata_authorship', 'analcatdata_bankruptcy', 'analcatdata_boxing1', 'analcatdata_boxing2', 'analcatdata_creditscore', 'analcatdata_cyyoung8092', 'analcatdata_cyyoung9302', 'analcatdata_dmft', 'analcatdata_fraud', 'analcatdata_germangss', 'analcatdata_happiness', 'analcatdata_japansolvent', 'analcatdata_lawsuit', 'ann_thyroid', 'appendicitis', 'australian', 'auto', 'backache', 'balance_scale', 'biomed', 'breast', 'breast_cancer', 'breast_cancer_wisconsin', 'breast_w', 'buggyCrx', 'bupa', 'calendarDOW', 'car', 'car_evaluation', 'cars', 'chess', 'churn', 'clean1', 'clean2', 'cleve', 'cleveland', 'cleveland_nominal', 'cloud', 'cmc', 'coil2000', 'colic', 'collins', 'confidence', 'connect_4', 'contraceptive', 'corral', 'credit_a', 'credit_g', 'crx', 'dermatology', 'diabetes', 'dis', 'dna', 'ecoli', 'fars', 'flags', 'flare', 'german', 'glass', 'glass2', 'haberman', 'hayes_roth', 'heart_c', 'heart_h', 'heart_statlog', 'hepatitis', 'horse_colic', 'house_votes_84', 'hungarian', 'hypothyroid', 'ionosphere', 'iris', 'irish', 'kddcup', 'kr_vs_kp', 'krkopt', 'labor', 'led24', 'led7', 'letter', 'lupus', 'lymphography', 'magic', 'mfeat_factors', 'mfeat_fourier', 'mfeat_karhunen', 'mfeat_morphological', 'mfeat_pixel', 'mfeat_zernike', 'mnist', 'mofn_3_7_10', 'molecular_biology_promoters', 'monk1', 'monk2', 'monk3', 'movement_libras', 'mushroom', 'mux6', 'new_thyroid', 'nursery', 'optdigits', 'page_blocks', 'parity5', 'parity5+5', 'pendigits', 'penguins', 'phoneme', 'pima', 'poker', 'postoperative_patient_data', 'prnn_crabs', 'prnn_fglass', 'prnn_synth', 'profb', 'ring', 'saheart', 'satimage', 'schizo', 'segmentation', 'shuttle', 'sleep', 'solar_flare_1', 'solar_flare_2', 'sonar', 'soybean', 'spambase', 'spect', 'spectf', 'splice', 'tae', 'texture', 'threeOf9', 'tic_tac_toe', 'tokyo1', 'twonorm', 'vehicle', 'vote', 'vowel', 'waveform_21', 'waveform_40', 'wdbc', 'wine_quality_red', 'wine_quality_white', 'wine_recognition', 'xd6', 'yeast']\n"
     ]
    }
   ],
   "source": [
    "from pmlb import fetch_data, classification_dataset_names\n",
    "\n",
    "print(classification_dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = []\n",
    "\n",
    "for dataset in classification_dataset_names:\n",
    "    X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "    if X.shape[0] >= 5000:\n",
    "        list_dataset.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(dataset):\n",
    "    X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "    y_min = np.unique(y).min()\n",
    "    if y_min == 1:\n",
    "        y -= 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "X, y = get_class_data(list_dataset[i])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  5,  7, 10, 12, 14, 16, 18, 21, 23, 25, 27, 29, 32, 34, 36,\n",
       "       38, 40, 43])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = train_X.shape[1]\n",
    "max_features = [int(x*N+1) for x in np.linspace(0.01, 2, num = 20)]\n",
    "np.unique(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestClassifier(max_features='sqrt',\n",
       "                                              n_estimators=1000, n_jobs=10),\n",
       "             n_jobs=2,\n",
       "             param_grid={'max_depth': array([ 1,  3,  5,  7, 10, 12, 14, 16, 18, 21, 23, 25, 27, 29, 32, 34, 36,\n",
       "       38, 40, 43])},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = train_X.shape[1]\n",
    "\n",
    "max_depth = np.unique([int(x*N + 1) for x in np.linspace(0.01, 2, num = 20)])\n",
    "\n",
    "grid = {'max_depth': max_depth}\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_features='sqrt', n_jobs = 10)\n",
    "\n",
    "\n",
    "rf_cv = GridSearchCV(estimator = rf, param_grid = grid, cv = 5, verbose=2,\n",
    "                     n_jobs = 2)\n",
    "rf_cv.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=23, max_features='sqrt', n_estimators=1000,\n",
       "                       n_jobs=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983333333333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "def other_scores(train_X, test_X, train_y, test_y):\n",
    "    \n",
    "    N = train_X.shape[1]\n",
    "    max_features = np.unique([int(x*N + 1) for x in np.linspace(0.01, 0.99, num = 5)])\n",
    "    grid = {'max_features': max_features}\n",
    "    rf = RandomForestClassifier(n_estimators=1000, max_features='sqrt', n_jobs = 10)\n",
    "    rf_cv = GridSearchCV(estimator = rf, param_grid = grid, cv = 5, verbose=2,\n",
    "                         n_jobs = 2)\n",
    "    \n",
    "    lasso  = LogisticRegressionCV(cv=5, penalty='l1',solver = 'saga', random_state=0)\n",
    "    ridge  = LogisticRegressionCV(cv=5, penalty='l2',solver = 'saga', random_state=0)\n",
    "    dt = DecisionTreeClassifier(max_depth=5)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    rf_cv.fit(train_X, train_y)\n",
    "    lasso.fit(train_X, train_y)\n",
    "    ridge.fit(train_X, train_y)\n",
    "    dt.fit(train_X, train_y)\n",
    "    scores = [x.score(test_X, test_y) for x in [rf_cv, ridge, lasso, dt]]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   55.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9983333333333333,\n",
       " 0.9488888888888889,\n",
       " 0.9488888888888889,\n",
       " 0.9966666666666667]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_scores(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  12 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done  25 out of  25 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done  25 out of  25 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27999061890109034"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(min_samples_leaf=10)\n",
    "dt_cv = GridSearchCV(estimator = dt, param_grid = grid, cv = 5, verbose=2,\n",
    "                    n_jobs = 20)\n",
    "dt_cv.fit(train_X, train_y)\n",
    "dt_cv.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=9, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=10,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionally interpretable super learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "\n",
    "def random_assignments(train_X, K=6):\n",
    "    data = {'index': range(len(train_X)), 'group':  np.random.choice(K, len(train_X)) }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "class BaseModel:\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.model = self.create_model()\n",
    "        if model_type not in range(1,7):\n",
    "            print(\"model_type should be in the interval [1, 6]\")\n",
    "\n",
    "    def create_model(self):\n",
    "        method_name = 'model_' + str(self.model_type)\n",
    "        method = getattr(self, method_name, lambda: \"nothing\")\n",
    "        return method()\n",
    "\n",
    "    # L1 penalty\n",
    "    def model_1(self):\n",
    "        return LogisticRegressionCV(cv=5, penalty='l1',solver = 'saga', random_state=0)\n",
    "\n",
    "    # l2 penalty\n",
    "    def model_2(self):\n",
    "        return LogisticRegressionCV(cv=5, penalty='l2', solver = 'saga', random_state=0)\n",
    "\n",
    "    # elastic net\n",
    "    def model_3(self):\n",
    "        return LogisticRegressionCV(cv=5, penalty='elasticnet', solver = 'saga', l1_ratios=[.5], random_state=0)\n",
    "\n",
    "    def model_4(self):\n",
    "        return DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "    def model_5(self):\n",
    "        return DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "    def model_6(self):\n",
    "        return DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.tree._classes.DecisionTreeClassifier"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = BaseModel(6)\n",
    "type(r.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "def fit_K_models(train_X, train_y, groups, model_types, K=6):\n",
    "    models = []\n",
    "    for k in range(K):\n",
    "        ind = groups[groups[\"group\"] == k].index.values\n",
    "        X = train_X[ind]\n",
    "        y = train_y[ind]\n",
    "        if len(ind) > 10:\n",
    "            base_model = BaseModel(model_types[k])\n",
    "            base_model.model.fit(X, y)\n",
    "            models.append(base_model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of fit K models use weights\n",
    "def fit_K_models_weights(train_X, train_y, groups, ind_base, model_types, K=6):\n",
    "    models = []\n",
    "    # this is a hack for now\n",
    "    for k in range(K):\n",
    "        ind = groups[groups[\"group\"] == k].index.values\n",
    "        ind = np.array(ind_base + list(ind))\n",
    "        X = train_X[ind]\n",
    "        y = train_y[ind]\n",
    "        if len(ind) > 10:\n",
    "            base_model = BaseModel(model_types[k])\n",
    "            base_model.model.fit(X, y)\n",
    "            models.append(base_model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(train_y)\n",
    "hack = {c: (np.where(train_y == c))[0] for c in classes}\n",
    "ind_base = [hack[c][0] for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = random_assignments(train_X, K=6)\n",
    "models = fit_K_models_weights(train_X, train_y, groups, ind_base, [1,2,3,4,5,6], K=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L is an array\n",
    "def compute_K_model_loss(X, y, models):\n",
    "    L = []\n",
    "    for i in range(len(models)):\n",
    "        y_hat = models[i].model.predict_proba(X)\n",
    "        print(i, y_hat.shape)\n",
    "        W = np.eye(y_hat.shape[1])[y] # to avoid the need for num_classes\n",
    "        loss = (-np.log(y_hat + 1e-8)*W).sum(1)\n",
    "        L.append(loss)\n",
    "    L = np.array(L)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (5400, 3)\n",
      "1 (5400, 3)\n",
      "2 (5400, 3)\n",
      "3 (5400, 3)\n",
      "4 (5400, 3)\n",
      "5 (5400, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 5400)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_K_model_loss(train_X, train_y, models).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "y_hat = models[i].model.predict_proba(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9596296296296296"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(y_hat, axis=1) == train_y).sum()/train_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(L, K):\n",
    "    JI_K = inv(np.ones((K, K)) - np.identity(K))\n",
    "    W = []\n",
    "    for i in range(L.shape[1]):\n",
    "        w_i = np.matmul(JI_K, L[:,i])\n",
    "        W.append(w_i)\n",
    "    return np.array(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_dataset(train_X, train_y, models):\n",
    "    K = len(models)\n",
    "    N = train_X.shape[0]\n",
    "    L = compute_K_model_loss(train_X, train_y, models)\n",
    "    W = compute_weights(L, K)\n",
    "    X_ext = []\n",
    "    y_ext = []\n",
    "    w_ext = []\n",
    "    for i in range(K):\n",
    "        X_ext.append(train_X.copy())\n",
    "        y_ext.append(i*np.ones(N))\n",
    "        w_ext.append(W[:, i])\n",
    "    X_ext = np.concatenate(X_ext, axis=0)\n",
    "    y_ext = np.concatenate(y_ext, axis=0)\n",
    "    w_ext = np.concatenate(w_ext, axis=0)\n",
    "    return X_ext, y_ext, w_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oracle_model(D_in, K, N):\n",
    "    \"\"\" Returns an oracle model\n",
    "    \n",
    "    The size of the hidden layer is a function of the\n",
    "    amount of training data\n",
    "    \"\"\"\n",
    "    H = int(2*np.log(N)**2)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(D_in, H),\n",
    "        nn.BatchNorm1d(H),\n",
    "        nn.ReLU(),\n",
    "        torch.nn.Linear(H, K))\n",
    "    return model\n",
    "#nn.Dropout(p=0.2),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(beta, f_hat, y, w):\n",
    "    y_hat = np.exp(beta*f_hat)\n",
    "    den = (np.exp(beta*f_hat)).sum(axis=1)\n",
    "    y_hat = np.array([y_hat[i]/den[i] for i in range(len(den))])\n",
    "    loss = w*((y * (1- y_hat)).sum(axis=1))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.tensor(1).float()\n",
    "f_hat = torch.tensor([[1, 2, 4], [1, 2, 3]]).float()\n",
    "y = torch.tensor([[0, 0, 1], [0, 0, 1]]).float()\n",
    "w = torch.tensor([1, 1]).float()\n",
    "#sofmax_loss(beta, f_hat, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2455)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_hat = beta*f_hat\n",
    "y_hat = F.softmax(f_hat, dim=1)\n",
    "loss = (y*(1-y_hat)).sum(dim=1)\n",
    "(w*loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounded_loss(beta, y_hat, y , w):\n",
    "    #y_hat = beta*y_hat\n",
    "    y_hat = F.softmax(y_hat, dim=1)\n",
    "    loss = (y*(1-y_hat)).sum(dim=1)\n",
    "    return (w*loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oracle_model(model, train_dl, K, learning_rate = 0.01, epochs=100):\n",
    "    beta = 1\n",
    "    wd=0 #0.0001\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "    KK = epochs//10 + 1\n",
    "    model.train()\n",
    "    for t in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y, w in train_dl:\n",
    "            x = x.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "            w = w.cuda().float()\n",
    "            y_onehot = torch.FloatTensor(y.shape[0], K).cuda()\n",
    "            y_onehot.zero_()\n",
    "            y_onehot = y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "            y_hat = model(x)\n",
    "            loss = bounded_loss(beta, y_hat, y_onehot , w)\n",
    "       \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()*y.size(0)\n",
    "            total += y.size(0)\n",
    "        #if t % KK == 0: print(total_loss/total)\n",
    "        print(total_loss/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasign_points(train_X, model):\n",
    "    x = torch.tensor(train_X).float()\n",
    "    y_hat = model(x.cuda())\n",
    "    _, pred = torch.max(y_hat, 1)\n",
    "    data = {'index': range(len(train_X)), 'group': pred.cpu().numpy()  }\n",
    "    return pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_groups(groups, models):\n",
    "    unique_models = groups.group.unique()\n",
    "    old2new = {x:i for i,x in enumerate(unique_models)}\n",
    "    ratios = []\n",
    "    model_types = [models[i].model_type for i in unique_models]\n",
    "    groups.group = np.array([old2new[x] for x in groups.group.values])\n",
    "    return groups, model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, oracle, models):\n",
    "    oracle.eval()\n",
    "    x = torch.tensor(X).float()\n",
    "    y = torch.tensor(y).float()\n",
    "    y_hat = oracle(x.cuda())\n",
    "    _, ass = torch.max(y_hat, 1)\n",
    "    preds = []\n",
    "    ys = []\n",
    "    for i in range(len(models)):\n",
    "        xx = x[ass==i]\n",
    "        yy = y[ass==i]\n",
    "        if len(xx) > 0:\n",
    "            pred = models[i].model.predict_proba(xx.cpu().numpy())\n",
    "            preds.append(pred)\n",
    "            ys.append(yy.cpu().numpy())\n",
    "            \n",
    "    preds = np.concatenate(preds)\n",
    "    ys = np.concatenate(ys)\n",
    "    logloss = log_loss(ys, preds)\n",
    "    acc = (np.argmax(preds, axis=1) == ys).sum()/ys.shape[0]\n",
    "    return logloss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_loss(X, y, model):\n",
    "    pred = model.model.predict_proba(X)\n",
    "    logloss = log_loss(y, pred)\n",
    "    acc = model.model.score(X, y)\n",
    "    return logloss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_assignments(train_X, K=6):\n",
    "    data = {'index': range(len(train_X)), 'group':  np.random.choice(K, len(train_X)) }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.0001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/data2/yinterian/tmp/\")\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 20)\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "X, y = get_class_data(list_dataset[i])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points 3750, number iterations 12\n",
      "classes [0 1]\n",
      "iter 0\n",
      "K is  6\n",
      "0 (3750, 2)\n",
      "1 (3750, 2)\n",
      "2 (3750, 2)\n",
      "3 (3750, 2)\n",
      "4 (3750, 2)\n",
      "5 (3750, 2)\n",
      "0.0642700120806694\n",
      "0.05760582163929939\n",
      "0.05391684174537659\n",
      "0.051554325968027115\n",
      "0.049794845283031464\n",
      "0.04823724180459976\n",
      "0.046701639890670776\n",
      "0.045280538499355316\n",
      "0.04418635740876198\n",
      "0.04340357705950737\n",
      "0.042683809995651245\n",
      "0.04186432436108589\n",
      "loss 0.24008368639426536 0.3121183038040852\n",
      "Accuracy 0.9176 0.9024\n",
      "best test_acc 0.9024\n",
      "iter 1\n",
      "K is  4\n",
      "0 (3750, 2)\n",
      "1 (3750, 2)\n",
      "2 (3750, 2)\n",
      "3 (3750, 2)\n",
      "0.13198190927505493\n",
      "0.10527495294809341\n",
      "0.08745112270116806\n",
      "0.07545369863510132\n",
      "0.06731664389371872\n",
      "0.06212976202368736\n",
      "0.05885540321469307\n",
      "0.05653737857937813\n",
      "0.05465400218963623\n",
      "0.05300024524331093\n",
      "0.051525913178920746\n",
      "0.05023519694805145\n",
      "loss 0.18766627030746383 0.3773213784591908\n",
      "Accuracy 0.9370666666666667 0.92\n",
      "best test_acc 0.92\n",
      "iter 2\n",
      "K is  3\n",
      "0 (3750, 2)\n",
      "1 (3750, 2)\n",
      "2 (3750, 2)\n",
      "0.19952869415283203\n",
      "0.14286810159683228\n",
      "0.10610603541135788\n",
      "0.08729595690965652\n",
      "0.07773634046316147\n",
      "0.0724235400557518\n",
      "0.06869156658649445\n",
      "0.0655771791934967\n",
      "0.06279414892196655\n",
      "0.0602414608001709\n",
      "0.058023639023303986\n",
      "0.056235551834106445\n",
      "loss 0.17539073146749634 0.35442457983752673\n",
      "Accuracy 0.964 0.952\n",
      "best test_acc 0.952\n",
      "iter 3\n",
      "K is  3\n",
      "0 (3750, 2)\n",
      "1 (3750, 2)\n",
      "2 (3750, 2)\n",
      "0.36019009351730347\n",
      "0.2223193645477295\n",
      "0.14227209985256195\n",
      "0.10219693928956985\n",
      "0.08243853598833084\n",
      "0.07264778763055801\n",
      "0.06772790849208832\n",
      "0.06508798897266388\n",
      "0.06350335478782654\n",
      "0.06243809685111046\n",
      "0.061671432107686996\n",
      "0.06108570098876953\n",
      "loss 0.20205217630797864 0.4508388466548591\n",
      "Accuracy 0.9589333333333333 0.9448\n",
      "best test_acc 0.952\n",
      "iter 4\n",
      "K is  2\n",
      "0 (3750, 2)\n",
      "1 (3750, 2)\n",
      "0.40556472539901733\n",
      "0.26076826453208923\n",
      "0.17211909592151642\n",
      "0.1214737817645073\n",
      "0.09597280621528625\n",
      "0.08367375284433365\n",
      "0.07750406116247177\n",
      "0.07416734099388123\n",
      "0.07219646871089935\n",
      "0.07090696692466736\n",
      "0.06997653841972351\n",
      "0.06925610452890396\n",
      "loss 0.13513088565985887 0.25760560297381696\n",
      "Accuracy 0.9666666666666667 0.9576\n",
      "best test_acc 0.9576\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   47.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset yeast K 2 ISL 0.9576 RF 0.9544 Ridge 0.8552 Lasso 0.8600 Cart 0.9304\n"
     ]
    }
   ],
   "source": [
    "K = 6\n",
    "groups = random_assignments(train_X, K)\n",
    "f = open('out2.log', 'w+')\n",
    "batch_size = 100000\n",
    "# number of iterations depends on the number of training points\n",
    "N = train_X.shape[0]\n",
    "N_iter = int(100/np.log(N))\n",
    "print(\"Number of training points %d, number iterations %d\" % (N, N_iter))\n",
    "\n",
    "best_train_acc = 0\n",
    "best_K = None\n",
    "best_test_acc = 0\n",
    "model_types = range(1,7)\n",
    "learning_rate = 0.01\n",
    "\n",
    "classes = np.unique(train_y)\n",
    "print(\"classes\", classes)\n",
    "hack = {c: (np.where(train_y == c))[0] for c in classes}\n",
    "ind_base = [hack[c][0] for c in classes]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"iter\", i)\n",
    "    models = fit_K_models_weights(train_X, train_y, groups, ind_base, model_types, K)\n",
    "    K = len(models)\n",
    "    print(\"K is \", K)\n",
    "    if K == 1:\n",
    "        models[0].model.fit(train_X, train_y)\n",
    "        train_loss, train_acc = compute_single_loss(train_X, train_y, models[0])\n",
    "        test_loss, test_acc = compute_single_loss(test_X, test_y, models[0])\n",
    "        if train_acc >= best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_K = K\n",
    "        break\n",
    "    \n",
    "    X_ext, y_ext, w_ext = create_extended_dataset(train_X, train_y, models)\n",
    "    train_ds = OracleDataset(X_ext, y_ext, w_ext)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = create_oracle_model(train_X.shape[1], K, N).cuda()\n",
    "    train_oracle_model(model, train_dl, K, learning_rate, N_iter)\n",
    "    groups = reasign_points(train_X, model)\n",
    "    if len(groups.group.unique()) < K:\n",
    "        K = len(groups.group.unique()) \n",
    "        groups, model_types = relabel_groups(groups, models)\n",
    "    train_loss, train_acc = compute_loss(train_X, train_y, model, models)\n",
    "        \n",
    "    test_loss, test_acc = compute_loss(test_X, test_y, model, models)\n",
    "    if train_acc >= best_train_acc:\n",
    "        best_train_acc = train_acc\n",
    "        best_test_acc = test_acc\n",
    "        best_K = K\n",
    "    print(\"loss\", train_loss, test_loss)\n",
    "    print(\"Accuracy\", train_acc, test_acc)\n",
    "    print(\"best test_acc\", best_test_acc)\n",
    "    \n",
    "scores = other_scores(train_X, test_X, train_y, test_y)\n",
    "model_str = [\"RF\", \"Ridge\", \"Lasso\", \"Cart\"]\n",
    "score_str = [\"%s %.4f\" % (s, score) for s,score in zip(model_str, scores)]\n",
    "score_str = \" \".join(score_str)\n",
    "results = \"dataset %s K %d ISL %.4f %s\"  %(dataset, best_K, best_test_acc, score_str)\n",
    "print(results)\n",
    "f.write(results)\n",
    "f.write('\\n')\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 21)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.7\n",
    "K = len(models)\n",
    "N = train_X.shape[0]\n",
    "n = int(p*N)\n",
    "idx = np.random.choice(N, n, replace=False)\n",
    "X = train_X[idx]\n",
    "Y = train_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3779, 3)\n",
      "1 (3779, 2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-bf4a51080a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_K_model_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-f0bffd3d78fc>\u001b[0m in \u001b[0;36mcompute_K_model_loss\u001b[0;34m(X, y, models)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# to avoild the need for num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "L = compute_K_model_loss(X, Y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-36d0b029d72d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_K_model_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-8afff79410b8>\u001b[0m in \u001b[0;36mcompute_K_model_loss\u001b[0;34m(train_X, train_y, models)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# to avoild the need for num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "L = compute_K_model_loss(X, Y, models)\n",
    "W = compute_weights(L, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
