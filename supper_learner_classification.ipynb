{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Classification Datasets for Super Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmlb import fetch_data, classification_dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = []\n",
    "\n",
    "for dataset in classification_dataset_names:\n",
    "    X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "    if X.shape[0] >= 5000:\n",
    "        list_dataset.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(dataset):\n",
    "    X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "    y_min = np.unique(y).min()\n",
    "    if y_min == 1:\n",
    "        y -= 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "def other_scores(train_X, test_X, train_y, test_y):\n",
    "    \n",
    "    N = train_X.shape[1]\n",
    "    max_features = np.unique([int(x*N + 1) for x in np.linspace(0.01, 0.99, num = 5)])\n",
    "    grid = {'max_features': max_features}\n",
    "    rf = RandomForestClassifier(n_estimators=1000, max_features='sqrt', n_jobs = 10)\n",
    "    rf_cv = GridSearchCV(estimator = rf, param_grid = grid, cv = 5, verbose=2,\n",
    "                         n_jobs = 2)\n",
    "    \n",
    "    lasso  = LogisticRegressionCV(cv=5, penalty='l1',solver = 'saga', random_state=0)\n",
    "    ridge  = LogisticRegressionCV(cv=5, penalty='l2',solver = 'saga', random_state=0)\n",
    "    dt = DecisionTreeClassifier(max_depth=5)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    rf_cv.fit(train_X, train_y)\n",
    "    lasso.fit(train_X, train_y)\n",
    "    ridge.fit(train_X, train_y)\n",
    "    dt.fit(train_X, train_y)\n",
    "    scores = [x.score(test_X, test_y) for x in [rf_cv, ridge, lasso, dt]]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other_scores(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionally interpretable super learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "class BaseModel:\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.model = self.create_model()\n",
    "        if model_type not in range(1,7):\n",
    "            print(\"model_type should be in the interval [1, 6]\")\n",
    "\n",
    "    def create_model(self):\n",
    "        method_name = 'model_' + str(self.model_type)\n",
    "        method = getattr(self, method_name, lambda: \"nothing\")\n",
    "        return method()\n",
    "\n",
    "    # L1 penalty\n",
    "    def model_1(self):\n",
    "        return LogisticRegressionCV(cv=5, penalty='l1',solver = 'saga', random_state=0)\n",
    "\n",
    "    # l2 penalty\n",
    "    def model_2(self):\n",
    "        return LogisticRegressionCV(cv=5, penalty='l2', solver = 'saga', random_state=0)\n",
    "\n",
    "    # elastic net\n",
    "    def model_3(self):\n",
    "        return LogisticRegressionCV(cv=5, penalty='elasticnet', solver = 'saga', l1_ratios=[.5], random_state=0)\n",
    "\n",
    "    def model_4(self):\n",
    "        return DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "    def model_5(self):\n",
    "        return DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "    def model_6(self):\n",
    "        return DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_initial_K_models(train_X, train_y, model_types):\n",
    "    models = []\n",
    "    N = train_X.shape[0]\n",
    "    n = int(3*N/np.log(N))\n",
    "    for k in range(len(model_types)):\n",
    "        ind = np.random.choice(N, n, replace=False)\n",
    "        X = train_X[ind]\n",
    "        y = train_y[ind]\n",
    "        if len(ind) > 10:\n",
    "            base_model = BaseModel(model_types[k])\n",
    "            base_model.model.fit(X, y)\n",
    "            models.append(base_model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_K_models(train_X, train_y, oracle, models,  idx_base, p=0.8):\n",
    "    # sample to address overfitting \n",
    "    N = train_X.shape[0]\n",
    "    #n = int(p*N)\n",
    "    #ind = np.random.choice(N, n, replace=False)\n",
    "    #X = train_X[ind]\n",
    "    #y = train_y[ind]\n",
    "    # assigning points using oracle\n",
    "    # this will be modified \n",
    "    x = torch.tensor(train_X).float()\n",
    "    y_hat = oracle(x.cuda())\n",
    "    W = F.softmax(0.5*y_hat, dim=1).cpu().detach().numpy()\n",
    "\n",
    "    model_types = [m.model_type for m in models]\n",
    "    models = []\n",
    "    for k in range(len(model_types)):\n",
    "        w = W[:,k]\n",
    "        if w.sum()/N > 0.015:\n",
    "            idx = w > 0.000001\n",
    "            idx = np.array(list(idx) + idx_base)\n",
    "            w = W[idx, k].copy()\n",
    "            X_k = train_X[idx]\n",
    "            y_k = train_y[idx]\n",
    "            base_model = BaseModel(model_types[k])\n",
    "            print(\"model_type=\", model_types[k], k)\n",
    "            base_model.model.fit(X_k, y_k, w)\n",
    "            models.append(base_model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L is an array\n",
    "def compute_K_model_loss(X, y, models):\n",
    "    L = []\n",
    "    for i in range(len(models)):\n",
    "        y_hat = models[i].model.predict_proba(X)\n",
    "        W = np.eye(y_hat.shape[1])[y] # to avoid the need for num_classes\n",
    "        loss = (-np.log(y_hat + 1e-8)*W).sum(1)\n",
    "        L.append(loss)\n",
    "    L = np.array(L)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(L, K):\n",
    "    JI_K = inv(np.ones((K, K)) - np.identity(K))\n",
    "    W = []\n",
    "    for i in range(L.shape[1]):\n",
    "        w_i = np.matmul(JI_K, L[:,i])\n",
    "        W.append(w_i)\n",
    "    return np.array(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_dataset(train_X, train_y, models, p=0.7):\n",
    "    # sample to address overfitting\n",
    "    K = len(models)\n",
    "    N = train_X.shape[0]\n",
    "    n = int(p*N)\n",
    "    idx = np.random.choice(N, n, replace=False)\n",
    "    X = train_X[idx]\n",
    "    Y = train_y[idx]\n",
    "    L = compute_K_model_loss(X, Y, models)\n",
    "    W = compute_weights(L, K)\n",
    "    X_ext = []\n",
    "    y_ext = []\n",
    "    w_ext = []\n",
    "    for i in range(K):\n",
    "        X_ext.append(X.copy())\n",
    "        y_ext.append(i*np.ones(n))\n",
    "        w_ext.append(W[:, i])\n",
    "    X_ext = np.concatenate(X_ext, axis=0)\n",
    "    y_ext = np.concatenate(y_ext, axis=0)\n",
    "    w_ext = np.concatenate(w_ext, axis=0)\n",
    "    return X_ext, y_ext, w_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oracle_model(D_in, K, N):\n",
    "    \"\"\" Returns an oracle model\n",
    "    \n",
    "    The size of the hidden layer is a function of the\n",
    "    amount of training data\n",
    "    \"\"\"\n",
    "    H = int(2*np.log(N)**2)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(D_in, H),\n",
    "        nn.BatchNorm1d(H),\n",
    "        nn.ReLU(),\n",
    "        torch.nn.Linear(H, K))\n",
    "    return model\n",
    "#nn.Dropout(p=0.2),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(beta, f_hat, y, w):\n",
    "    y_hat = np.exp(beta*f_hat)\n",
    "    den = (np.exp(beta*f_hat)).sum(axis=1)\n",
    "    y_hat = np.array([y_hat[i]/den[i] for i in range(len(den))])\n",
    "    loss = w*((y * (1- y_hat)).sum(axis=1))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounded_loss(beta, y_hat, y , w):\n",
    "    #y_hat = beta*y_hat\n",
    "    y_hat = F.softmax(y_hat, dim=1)\n",
    "    loss = (y*(1-y_hat)).sum(dim=1)\n",
    "    return (w*loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, K, learning_rate = 0.01, epochs=100):\n",
    "    beta = 1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00001)\n",
    "    KK = epochs//10 + 1\n",
    "    model.train()\n",
    "    for t in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y, w in train_dl:\n",
    "            x = x.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "            w = w.cuda().float()\n",
    "            y_onehot = torch.FloatTensor(y.shape[0], K).cuda()\n",
    "            y_onehot.zero_()\n",
    "            y_onehot = y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "            y_hat = model(x)\n",
    "            loss = bounded_loss(beta, y_hat, y_onehot , w)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()*y.size(0)\n",
    "            total += y.size(0)\n",
    "        if t % KK == 0: print(\"epoch %d loss %.4f\" % (t, total_loss/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasign_points(train_X, model):\n",
    "    x = torch.tensor(train_X).float()\n",
    "    y_hat = model(x.cuda())\n",
    "    _, pred = torch.max(y_hat, 1)\n",
    "    data = {'index': range(len(train_X)), 'group': pred.cpu().numpy()  }\n",
    "    return pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_groups(groups, models):\n",
    "    unique_models = groups.group.unique()\n",
    "    old2new = {x:i for i,x in enumerate(unique_models)}\n",
    "    ratios = []\n",
    "    model_types = [models[i].model_type for i in unique_models]\n",
    "    groups.group = np.array([old2new[x] for x in groups.group.values])\n",
    "    return groups, model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, oracle, models):\n",
    "    oracle.eval()\n",
    "    x = torch.tensor(X).float()\n",
    "    y = torch.tensor(y).float()\n",
    "    y_hat = oracle(x.cuda())\n",
    "    _, ass = torch.max(y_hat, 1)\n",
    "    preds = []\n",
    "    ys = []\n",
    "    for i in range(len(models)):\n",
    "        xx = x[ass==i]\n",
    "        yy = y[ass==i]\n",
    "        if len(xx) > 0:\n",
    "            pred = models[i].model.predict_proba(xx.cpu().numpy())\n",
    "            preds.append(pred)\n",
    "            ys.append(yy.cpu().numpy())\n",
    "            \n",
    "    preds = np.concatenate(preds)\n",
    "    ys = np.concatenate(ys)\n",
    "    logloss = log_loss(ys, preds)\n",
    "    acc = (np.argmax(preds, axis=1) == ys).sum()/ys.shape[0]\n",
    "    return logloss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_loss(X, y, model):\n",
    "    pred = model.model.predict_proba(X)\n",
    "    logloss = log_loss(y, pred)\n",
    "    acc = model.model.score(X, y)\n",
    "    return logloss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_models(train_X, train_y, valid_X, valid_y):\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "    best_model_type = 0\n",
    "    for k in range(1,7):\n",
    "        base_model = BaseModel(k)\n",
    "        base_model.model.fit(train_X, train_y)\n",
    "        valid_acc = base_model.model.score(valid_X, valid_y)\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model_type = k\n",
    "            best_model = base_model.model\n",
    "    return best_valid_acc, best_model, [best_model_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_assignments(train_X, K=6):\n",
    "    data = {'index': range(len(train_X)), 'group':  np.random.choice(K, len(train_X)) }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.0001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/data2/yinterian/tmp/\")\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(dataset):\n",
    "    X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "    y_min = np.unique(y).min()\n",
    "    if y_min == 1:\n",
    "        y -= 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datatest_split(dataset, state):\n",
    "    X, y = get_class_data(dataset)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=state, test_size = 0.3)\n",
    "    valid_X, test_X, valid_y, test_y = train_test_split(test_X, test_y, random_state=state, test_size =0.5)\n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "    valid_X = scaler.transform(valid_X)\n",
    "    return train_X, valid_X, test_X, train_y, valid_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_sample_from_each_class(train_X, train_y):\n",
    "    classes = np.unique(train_y)\n",
    "    idx_by_class = {c: np.where(train_y == c)[0] for c in classes}\n",
    "    return [idx_by_class[c][0] for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = [ 'adult', 'agaricus_lepiota', 'churn', 'clean2', 'magic', 'mushroom', 'phoneme', 'ring',\n",
    " 'twonorm', 'waveform_21', 'waveform_40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#churn has a problem i=2 'ann_thyroid' has issiues i=7, i=10\n",
    "i=8 is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adult',\n",
       " 'agaricus_lepiota',\n",
       " 'ann_thyroid',\n",
       " 'churn',\n",
       " 'clean2',\n",
       " 'coil2000',\n",
       " 'connect_4',\n",
       " 'fars',\n",
       " 'kddcup',\n",
       " 'krkopt',\n",
       " 'letter',\n",
       " 'magic',\n",
       " 'mnist',\n",
       " 'mushroom',\n",
       " 'nursery',\n",
       " 'optdigits',\n",
       " 'page_blocks',\n",
       " 'pendigits',\n",
       " 'phoneme',\n",
       " 'poker',\n",
       " 'ring',\n",
       " 'satimage',\n",
       " 'shuttle',\n",
       " 'sleep',\n",
       " 'texture',\n",
       " 'twonorm',\n",
       " 'waveform_21',\n",
       " 'waveform_40']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 0\n",
    "dataset = list_dataset[12]\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = get_datatest_split(dataset, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 784)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_acc, best_model, best_model_types = baseline_models(train_X, train_y, valid_X, valid_y)\n",
    "best_test_acc = best_model.score(test_X, test_y)\n",
    "best_single_model = best_test_acc\n",
    "print(\"best valid acc %.3f best model type %d\" % (best_valid_acc, best_model_types[0]))\n",
    "best_oracle = None\n",
    "best_models = [best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "# number of iterations depends on the number of training points\n",
    "N = train_X.shape[0]\n",
    "N_iter = int(3000/np.log(N)**2)\n",
    "print(\"Number of training points %d, number iterations %d\" % (N, N_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [x for x in range(1,7)]\n",
    "K = len(model_types)\n",
    "INIT_FLAG = True\n",
    "learning_rate = 0.15\n",
    "best_train_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_base = get_a_sample_from_each_class(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 K is 10\n",
      "epoch 0 loss 0.0463\n",
      "epoch 4 loss 0.0416\n",
      "epoch 8 loss 0.0399\n",
      "epoch 12 loss 0.0398\n",
      "epoch 16 loss 0.0392\n",
      "epoch 20 loss 0.0388\n",
      "epoch 24 loss 0.0385\n",
      "epoch 28 loss 0.0383\n",
      "epoch 32 loss 0.0386\n",
      "epoch 0 loss 0.0460\n",
      "epoch 4 loss 0.0417\n",
      "epoch 8 loss 0.0406\n",
      "epoch 12 loss 0.0400\n",
      "epoch 16 loss 0.0399\n",
      "epoch 20 loss 0.0395\n",
      "epoch 24 loss 0.0393\n",
      "epoch 28 loss 0.0391\n",
      "epoch 32 loss 0.0389\n",
      "loss 0.39505179190631023 0.42826165029902513\n",
      "Accuracy 0.8445996695208051 0.8412197686645636\n",
      "best test_acc 0.8412197686645636\n",
      "model_type= 1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinterian/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type= 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinterian/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type= 6 5\n",
      "model_type= 6 7\n",
      "model_type= 6 8\n",
      "Iteration 2 K is 5\n",
      "epoch 0 loss 1.2493\n",
      "epoch 4 loss 0.1971\n",
      "epoch 8 loss 0.1971\n",
      "epoch 12 loss 0.1971\n",
      "epoch 16 loss 0.1971\n",
      "epoch 20 loss 0.1971\n",
      "epoch 24 loss 0.1971\n",
      "epoch 28 loss 0.1971\n",
      "epoch 32 loss 0.1971\n",
      "epoch 0 loss 1.3192\n",
      "epoch 4 loss 0.1946\n",
      "epoch 8 loss 0.1946\n",
      "epoch 12 loss 0.1946\n",
      "epoch 16 loss 0.1946\n",
      "epoch 20 loss 0.1946\n",
      "epoch 24 loss 0.1946\n",
      "epoch 28 loss 0.1946\n",
      "epoch 32 loss 0.1946\n",
      "loss 0.9824008775006432 0.9751893331075452\n",
      "Accuracy 0.6491662911221271 0.6519453207150369\n",
      "best test_acc 0.8412197686645636\n",
      "model_type= 1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinterian/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 K is 10\n",
      "epoch 0 loss 0.0440\n",
      "epoch 4 loss 0.0400\n",
      "epoch 8 loss 0.0390\n",
      "epoch 12 loss 0.0385\n",
      "epoch 16 loss 0.0384\n",
      "epoch 20 loss 0.0384\n",
      "epoch 24 loss 0.0381\n",
      "epoch 28 loss 0.0379\n",
      "epoch 32 loss 0.0380\n",
      "epoch 0 loss 0.0445\n",
      "epoch 4 loss 0.0393\n",
      "epoch 8 loss 0.0382\n",
      "epoch 12 loss 0.0380\n",
      "epoch 16 loss 0.0375\n",
      "epoch 20 loss 0.0373\n",
      "epoch 24 loss 0.0375\n",
      "epoch 28 loss 0.0373\n",
      "epoch 32 loss 0.0369\n",
      "loss 0.3748272541732972 0.38869545835796626\n",
      "Accuracy 0.8476791347453808 0.8398177357167893\n",
      "best test_acc 0.8398177357167893\n",
      "model_type= 5 4\n",
      "model_type= 6 5\n",
      "model_type= 6 6\n",
      "model_type= 6 8\n",
      "Iteration 4 K is 4\n",
      "epoch 0 loss 2.3410\n",
      "epoch 4 loss 1.4851\n",
      "epoch 8 loss 1.4445\n",
      "epoch 12 loss 1.4093\n",
      "epoch 16 loss 1.3895\n",
      "epoch 20 loss 1.3690\n",
      "epoch 24 loss 1.3467\n",
      "epoch 28 loss 1.3325\n",
      "epoch 32 loss 1.3192\n",
      "epoch 0 loss 2.3673\n",
      "epoch 4 loss 1.4951\n",
      "epoch 8 loss 1.4277\n",
      "epoch 12 loss 1.4050\n",
      "epoch 16 loss 1.3826\n",
      "epoch 20 loss 1.3616\n",
      "epoch 24 loss 1.3540\n",
      "epoch 28 loss 1.3295\n",
      "epoch 32 loss 1.3292\n",
      "loss 10.067972899853416 9.975447511183457\n",
      "Accuracy 0.7085023283761455 0.7111812127584998\n",
      "best test_acc 0.8398177357167893\n",
      "model_type= 5 0\n",
      "model_type= 6 1\n",
      "model_type= 6 2\n",
      "model_type= 6 3\n",
      "Iteration 5 K is 4\n",
      "epoch 0 loss 1.7446\n",
      "epoch 4 loss 1.3774\n",
      "epoch 8 loss 1.3830\n",
      "epoch 12 loss 1.3684\n",
      "epoch 16 loss 1.3743\n",
      "epoch 20 loss 1.3679\n",
      "epoch 24 loss 1.3703\n",
      "epoch 28 loss 1.3645\n",
      "epoch 32 loss 1.3639\n",
      "epoch 0 loss 1.7408\n",
      "epoch 4 loss 1.3908\n",
      "epoch 8 loss 1.3722\n",
      "epoch 12 loss 1.3713\n",
      "epoch 16 loss 1.3686\n",
      "epoch 20 loss 1.3634\n",
      "epoch 24 loss 1.3601\n",
      "epoch 28 loss 1.3573\n",
      "epoch 32 loss 1.3557\n",
      "loss 10.101697106938726 10.278100651692665\n",
      "Accuracy 0.7075259125732312 0.7024185068349106\n",
      "best test_acc 0.8398177357167893\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset magic K 8 ISL 0.8398 RF 0.8829 Ridge 0.7883 Lasso 0.7883 Cart 0.8360\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    if not INIT_FLAG:\n",
    "        models = fit_K_models(train_X, train_y, oracle, models, idx_base, p=0.9)\n",
    "        if len(models) == 1:\n",
    "            INIT_FLAG = True\n",
    "    if INIT_FLAG:\n",
    "        model_types = [1,1,1,4,5,6,6,6,6,6]\n",
    "        models = fit_initial_K_models(train_X, train_y, model_types)\n",
    "        INIT_FLAG = False\n",
    "    \n",
    "    K = len(models)\n",
    "    print(\"Iteration %d K is %d\" % (i+1, K))\n",
    "    if K == 1:\n",
    "        INIT_FLAG = True\n",
    "    \n",
    "    if not INIT_FLAG:\n",
    "        X_ext, y_ext, w_ext = create_extended_dataset(train_X, train_y, models, p=0.7)\n",
    "        train_ds = OracleDataset(X_ext, y_ext, w_ext)\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        oracle = create_oracle_model(train_X.shape[1], K, N).cuda()\n",
    "        train_model(oracle, train_dl, K, learning_rate, N_iter)\n",
    "\n",
    "    if not INIT_FLAG:\n",
    "        train_loss, train_acc = compute_loss(train_X, train_y, oracle, models)\n",
    "        valid_loss, valid_acc = compute_loss(valid_X, valid_y, oracle, models)\n",
    "        test_loss, test_acc = compute_loss(test_X, test_y, oracle, models)\n",
    "    \n",
    "    \n",
    "    if K == 1:\n",
    "        models[0].model.fit(train_X, train_y)\n",
    "        train_loss, train_acc = compute_single_loss(train_X, train_y, models[0])\n",
    "        test_loss, test_acc = compute_single_loss(test_X, test_y, models[0])\n",
    "        if train_acc >= best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_K = K\n",
    "        break\n",
    "    \n",
    "    X_ext, y_ext, w_ext = create_extended_dataset(train_X, train_y, models)\n",
    "    train_ds = OracleDataset(X_ext, y_ext, w_ext)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = create_oracle_model(train_X.shape[1], K, N).cuda()\n",
    "    train_model(model, train_dl, K, learning_rate, N_iter)\n",
    "    groups = reasign_points(train_X, model)\n",
    "    if len(groups.group.unique()) < K:\n",
    "        K = len(groups.group.unique()) \n",
    "        groups, model_types = relabel_groups(groups, models)\n",
    "    train_loss, train_acc = compute_loss(train_X, train_y, model, models)\n",
    "        \n",
    "    test_loss, test_acc = compute_loss(test_X, test_y, model, models)\n",
    "    if train_acc >= best_train_acc:\n",
    "        best_train_acc = train_acc\n",
    "        best_test_acc = test_acc\n",
    "        best_K = K\n",
    "    print(\"loss\", train_loss, test_loss)\n",
    "    print(\"Accuracy\", train_acc, test_acc)\n",
    "    print(\"best test_acc\", best_test_acc)\n",
    "    \n",
    "scores = other_scores(train_X, test_X, train_y, test_y)\n",
    "model_str = [\"RF\", \"Ridge\", \"Lasso\", \"Cart\"]\n",
    "score_str = [\"%s %.4f\" % (s, score) for s,score in zip(model_str, scores)]\n",
    "score_str = \" \".join(score_str)\n",
    "results = \"dataset %s K %d ISL %.4f %s\"  %(dataset, best_K, best_test_acc, score_str)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset adult K 3 ISL 0.8534 RF 0.8612 Ridge 0.8250 Lasso 0.8252 Cart 0.8525\n",
    "dataset agaricus_lepiota K 3 ISL 0.9959 RF 1.0000 Ridge 0.9615 Lasso 0.9615 Cart 0.9795\n",
    "dataset ann_thyroid K 2 ISL 0.9954 RF 0.9972 Ridge 0.9546 Lasso 0.9546 Cart 0.9981\n",
    "dataset churn K 6 ISL 0.9480 RF 0.9667 Ridge 0.8707 Lasso 0.8707 Cart 0.9373\n",
    "dataset clean2 K 1 ISL 1.0000 RF 1.0000 Ridge 1.0000 Lasso 1.0000 Cart 1.0000\n",
    "dataset coil2000 K 3 ISL 0.9362 RF 0.9261 Ridge 0.9383 Lasso 0.9383 Cart 0.9362\n",
    "dataset connect_4 K 5 ISL 0.7068 RF 0.8170 Ridge 0.6649 Lasso 0.6649 Cart 0.6805\n",
    "dataset krkopt K 3 ISL 0.3488 RF 0.8491 Ridge 0.2837 Lasso 0.2818 Cart 0.3443\n",
    "dataset krkopt K 5 ISL 0.3699 RF 0.8475 Ridge 0.2837 Lasso 0.2818 Cart 0.3443\n",
    "dataset magic K 8 ISL 0.8398 RF 0.8829 Ridge 0.7883 Lasso 0.7883 Cart 0.8360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
