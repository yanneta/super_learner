{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmlb import fetch_data, regression_dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionally interpretable super learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_assignments(train_X, K=6):\n",
    "    data = {'index': range(len(train_X)), 'group':  np.random.choice(K, len(train_X)) }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "class BaseModel:\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.model = self.create_model()\n",
    "        if model_type not in range(1,7):\n",
    "            print(\"model_type should be in the interval [1, 6]\")\n",
    "\n",
    "    def create_model(self):\n",
    "        method_name = 'model_' + str(self.model_type)\n",
    "        method = getattr(self, method_name, lambda: \"nothing\")\n",
    "        return method()\n",
    "\n",
    "    def model_1(self):\n",
    "        return RidgeCV(cv=5, alphas=alphas)\n",
    "\n",
    "    def model_2(self):\n",
    "        return ElasticNetCV(cv=5, random_state=0, l1_ratio=0.5)\n",
    "\n",
    "    def model_3(self):\n",
    "        return ElasticNetCV(cv=5, random_state=0, l1_ratio=1)\n",
    "    # max_features=0.9\n",
    "    def model_4(self):\n",
    "        return DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "    def model_5(self):\n",
    "        return DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "    def model_6(self):\n",
    "        return DecisionTreeRegressor(max_depth=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(train_X, train_y, m_type):\n",
    "    N = train_X.shape[0]\n",
    "    n = int(2.5*N/np.log(N))\n",
    "    ind = np.random.choice(N, n, replace=False)\n",
    "    X = train_X[ind]\n",
    "    y = train_y[ind]\n",
    "    base_model = BaseModel(m_type)\n",
    "    base_model.model.fit(X, y)\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_initial_K_models(train_X, train_y, model_types):\n",
    "    models = []\n",
    "    N = train_X.shape[0]\n",
    "    n = int(3*N/np.log(N))\n",
    "    for k in range(len(model_types)):\n",
    "        ind = np.random.choice(N, n, replace=False)\n",
    "        X = train_X[ind]\n",
    "        y = train_y[ind]\n",
    "        if len(ind) > 10:\n",
    "            base_model = BaseModel(model_types[k])\n",
    "            base_model.model.fit(X, y)\n",
    "            models.append(base_model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 4, 8, 16, 32, 64, 132]\n",
    "\n",
    "def fit_K_models(train_X, train_y, oracle, models, p=0.8):\n",
    "    # sample to address overfitting \n",
    "    N = train_X.shape[0]\n",
    "    n = int(p*N)\n",
    "    ind = np.random.choice(N, n, replace=False)\n",
    "    X = train_X[ind]\n",
    "    y = train_y[ind]\n",
    "    # assigning points using oracle\n",
    "    # this will be modified \n",
    "    oracle.eval()\n",
    "    x = torch.tensor(X).float()\n",
    "    y_hat = oracle(x.cuda())\n",
    "    W = F.softmax(0.1*y_hat, dim=1).cpu().detach().numpy()\n",
    "                \n",
    "    model_types = [m.model_type for m in models]\n",
    "    models = []\n",
    "    for k in range(len(model_types)):\n",
    "        w = W[:,k]\n",
    "        if w.sum()/n > 0.015:\n",
    "            idx = w > 0.000001\n",
    "            w = W[idx, k].copy() \n",
    "            X_k = X[idx]\n",
    "            y_k = y[idx]\n",
    "            print(\"model_type\", model_types[k])\n",
    "            base_model = BaseModel(model_types[k])\n",
    "            base_model.model.fit(X_k, y_k, w)\n",
    "            models.append(base_model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_K_model_loss(train_X, train_y, models):\n",
    "    L = []\n",
    "    for i in range(len(models)):\n",
    "        loss = (models[i].model.predict(train_X) - train_y)**2\n",
    "        L.append(loss)\n",
    "    L = np.array(L)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(L, K):\n",
    "    JI_K = inv(np.ones((K, K)) - np.identity(K))\n",
    "    W = []\n",
    "    for i in range(L.shape[1]):\n",
    "        w_i = np.matmul(JI_K, L[:,i])\n",
    "        W.append(w_i)\n",
    "    return np.array(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_dataset(train_X, train_y, models, p=0.7):\n",
    "    # sample to address overfitting\n",
    "    K = len(models)\n",
    "    N = train_X.shape[0]\n",
    "    n = int(p*N)\n",
    "    idx = np.random.choice(N, n, replace=False)\n",
    "    X = train_X[idx]\n",
    "    Y = train_y[idx]\n",
    "    L = compute_K_model_loss(X, Y, models)\n",
    "    W = compute_weights(L, K)\n",
    "    X_ext = []\n",
    "    y_ext = []\n",
    "    w_ext = []\n",
    "    for i in range(K):\n",
    "        X_ext.append(X.copy())\n",
    "        y_ext.append(i*np.ones(n))\n",
    "        w_ext.append(W[:, i])\n",
    "    X_ext = np.concatenate(X_ext, axis=0)\n",
    "    y_ext = np.concatenate(y_ext, axis=0)\n",
    "    w_ext = np.concatenate(w_ext, axis=0)\n",
    "    return X_ext, y_ext, w_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oracle_model(D_in, K, N):\n",
    "    \"\"\" Returns an oracle model\n",
    "    \n",
    "    The size of the hidden layer is a function of the\n",
    "    amount of training data\n",
    "    \"\"\"\n",
    "    H = np.minimum(int(2*np.log(N)**2), 150)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(D_in, H),\n",
    "        nn.BatchNorm1d(H),\n",
    "        nn.ReLU(),\n",
    "        torch.nn.Linear(H, K))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(beta, f_hat, y, w):\n",
    "    y_hat = np.exp(beta*f_hat)\n",
    "    den = (np.exp(beta*f_hat)).sum(axis=1)\n",
    "    y_hat = np.array([y_hat[i]/den[i] for i in range(len(den))])\n",
    "    loss = w*((y * (1- y_hat)).sum(axis=1))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounded_loss(beta, y_hat, y , w):\n",
    "    #y_hat = beta*y_hat\n",
    "    y_hat = F.softmax(y_hat, dim=1)\n",
    "    loss = (y*(1-y_hat)).sum(dim=1)\n",
    "    return (w*loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, K, learning_rate = 0.01, epochs=100):\n",
    "    beta = 1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    KK = epochs//10 + 1\n",
    "    model.train()\n",
    "    for t in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y, w in train_dl:\n",
    "            x = x.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "            w = w.cuda().float()\n",
    "            y_onehot = torch.FloatTensor(y.shape[0], K).cuda()\n",
    "            y_onehot.zero_()\n",
    "            y_onehot = y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "            y_hat = model(x)\n",
    "            loss = bounded_loss(beta, y_hat, y_onehot , w)\n",
    "       \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()*y.size(0)\n",
    "            total += y.size(0)\n",
    "        if t % KK == 0: print(\"epoch %d loss %.4f\" % (t, total_loss/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, oracle, models):\n",
    "    oracle.eval()\n",
    "    x = torch.tensor(X).float()\n",
    "    y = torch.tensor(y).float()\n",
    "    y_hat = oracle(x.cuda())\n",
    "    _, ass = torch.max(y_hat, 1)\n",
    "    preds = []\n",
    "    ys = []\n",
    "    k = 0\n",
    "    #print(ass)\n",
    "    for i in range(len(models)):\n",
    "        xx = x[ass==i]\n",
    "        yy = y[ass==i]\n",
    "        if len(xx) > 0:\n",
    "            k =+1\n",
    "            pred = models[i].model.predict(xx.cpu().numpy())\n",
    "            preds.append(pred)\n",
    "            ys.append(yy.cpu().numpy())\n",
    "\n",
    "    if k==1:\n",
    "        preds, ys = preds[0], ys[0]\n",
    "    else:\n",
    "        preds = np.hstack(preds)\n",
    "        ys = np.hstack(ys)\n",
    "    r2 = r2_score(ys, preds)\n",
    "    res = (ys - preds)**2\n",
    "    return res.mean(), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_loss(X, y, model):\n",
    "    pred = model.model.predict(X)\n",
    "    r2 = r2_score(y, pred)\n",
    "    res = (y - pred)**2\n",
    "    return res.mean(), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.0001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_models(train_X, train_y, valid_X, valid_y):\n",
    "    best_model = None\n",
    "    best_valid_r2 = 0\n",
    "    best_model_type = 0\n",
    "    for k in range(1,7):\n",
    "        base_model = BaseModel(k)\n",
    "        base_model.model.fit(train_X, train_y)\n",
    "        valid_r2 = base_model.model.score(valid_X, valid_y)\n",
    "        if valid_r2 > best_valid_r2:\n",
    "            best_valid_r2 = valid_r2\n",
    "            best_model_type = k\n",
    "            best_model = base_model.model\n",
    "    return best_valid_r2, best_model, [best_model_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/data2/yinterian/tmp/\")\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_dl, K, lr_low=1e-5, lr_high=1, epochs=10):\n",
    "    losses = []\n",
    "    p = PATH/\"mode_tmp.pth\"\n",
    "    save_model(model, str(p))\n",
    "    iterations = epochs * len(train_dl)\n",
    "    delta = (lr_high - lr_low)/iterations\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    for i in range(epochs):\n",
    "        for x, y, w in train_dl:\n",
    "            optimizer = get_optimizer(model, lr=lrs[ind])\n",
    "            x = x.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "            w = w.cuda().float()\n",
    "            y_onehot = torch.FloatTensor(y.shape[0], K).cuda()\n",
    "            y_onehot.zero_()\n",
    "            y_onehot = y_onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "            y_hat = model(x)\n",
    "            loss = bounded_loss(beta, y_hat, y_onehot , w)\n",
    "       \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ind +=1\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    load_model(model, str(p))\n",
    "    return lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_LR_range_finder(train_X, train_y, K=6):\n",
    "    groups = random_assignments(train_X, K)\n",
    "    models = fit_K_models(train_X, train_y, groups, model_types)\n",
    "    K = len(models)\n",
    "    print(\"models\")\n",
    "    X_ext, y_ext, w_ext = create_extended_dataset(train_X, train_y, models)\n",
    "    train_ds = OracleDataset(X_ext, y_ext, w_ext)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    model = create_oracle_model(train_X.shape[1], K, N).cuda()\n",
    "    lrs, losses = LR_range_finder(model, train_dl, K, lr_low=1e-5, lr_high=0.5)\n",
    "    return lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"1028_SWD\"\n",
    "X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_map = {\"1028_SWD\": 0.15, \"1029_LEV\" :0.15, \"1030_ERA\": 0.15, \"1191_BNG_pbc\": 0.02,\n",
    "         \"1193_BNG_lowbwt\": 0.1, \"1196_BNG_pharynx\": 0.015, \"1199_BNG_echoMonths\": 0.3,\n",
    "         \"1203_BNG_pwLinear\": 0.05, \"1595_poker\": 0.01, \"1201_BNG_breastTumor\": 0.05, \"197_cpu_act\": 0.2,\n",
    "         \"201_pol\": 0.15, \"215_2dplanes\": 0.1, \"218_house_8L\": 0.05, \"225_puma8NH\": 0.15,\n",
    "         \"227_cpu_small\":0.15, \"294_satellite_image\": 0.15, \"344_mv\": 0.1,\n",
    "          \"4544_GeographicalOriginalofMusic\": 0.15, \"503_wind\": 0.1, \"529_pollen\": 0.1,\n",
    "         \"537_houses\": 0.15, \"562_cpu_small\": 0.15, \"564_fried\": 0.1, \"573_cpu_act\": 0.15,\n",
    "         \"574_house_16H\": 0.15, \"583_fri_c1_1000_50\": 0.15, \"586_fri_c3_1000_25\": 0.15 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199_BNG_echoMonths (13996, 9)\n"
     ]
    }
   ],
   "source": [
    "# difficult problems 5, 7, 22\n",
    "dataset = \"1201_BNG_breastTumor\"\n",
    "dataset = \"1199_BNG_echoMonths\"\n",
    "\n",
    "state=2\n",
    "X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=state, test_size = 0.2)\n",
    "valid_X, test_X, valid_y, test_y = train_test_split(test_X, test_y, random_state=state,\n",
    "                                                            test_size =0.5) \n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "valid_X = scaler.transform(valid_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "print(dataset, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_r2, best_model, best_model_types = baseline_models(train_X, train_y,\n",
    "                                                              valid_X, valid_y)\n",
    "best_test_r2 = best_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.458727912003307,\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=6, max_features=None,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       " [6],\n",
       " 0.44692103696475427)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_valid_r2, best_model, best_model_types, best_test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datatest_split(dataset, state):\n",
    "    X, y = fetch_data(dataset, return_X_y=True, local_cache_dir='/data2/yinterian/pmlb/')\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=state, test_size = 0.3)\n",
    "    valid_X, test_X, valid_y, test_y = train_test_split(test_X, test_y, random_state=state, test_size =0.5)\n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "    valid_X = scaler.transform(valid_X)\n",
    "    return train_X, valid_X, test_X, train_y, valid_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 2.6370\n",
      "epoch 5 loss 1.8196\n",
      "epoch 10 loss 1.7841\n",
      "epoch 15 loss 1.7534\n",
      "epoch 20 loss 1.7317\n",
      "epoch 25 loss 1.7367\n",
      "epoch 30 loss 1.7355\n",
      "epoch 35 loss 1.7220\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "learning_rate = 0.15\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = get_datatest_split(\"225_puma8NH\", state)\n",
    "model_types = [x for x in range(1,7)] \n",
    "models = fit_initial_K_models(train_X, train_y, model_types)\n",
    "K = len(models)\n",
    "\n",
    "N = train_X.shape[0]\n",
    "N_iter = int(3000/np.log(N)**2)\n",
    "\n",
    "batch_size = 100000\n",
    "\n",
    "X_ext, y_ext, w_ext = create_extended_dataset(train_X, train_y, models, p=0.9)\n",
    "train_ds = OracleDataset(X_ext, y_ext, w_ext)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "oracle = create_oracle_model(train_X.shape[1], K, N).cuda()\n",
    "train_model(oracle, train_dl, K, learning_rate, N_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_X[:10,]\n",
    "\n",
    "x = torch.tensor(X).float()\n",
    "y_hat = oracle(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = F.softmax(y_hat, dim=1).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[idx].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(state, selected_datasets):\n",
    "    for dataset in selected_datasets:\n",
    "        learning_rate = lr_map.get(dataset, 0.15)\n",
    "        train_X, valid_X, test_X, train_y, valid_y, test_y = get_datatest_split(dataset, state)\n",
    "\n",
    "        best_valid_r2, best_model, best_model_types = baseline_models(train_X, train_y, valid_X, valid_y)\n",
    "        best_test_r2 = best_model.score(test_X, test_y)\n",
    "        print(\"best valid R^2 %.3f best model type %d\" % (best_valid_r2, best_model_types[0]))\n",
    "        best_oracle = None\n",
    "        best_models = [best_model] \n",
    "\n",
    "\n",
    "        batch_size = 100000\n",
    "        # number of iterations depends on the number of training points\n",
    "        N = train_X.shape[0]\n",
    "        N_iter = int(3000/np.log(N)**2)\n",
    "        print(\"Number of training points %d, number iterations %d\" % (N, N_iter))\n",
    "\n",
    "        INIT_FLAG = True\n",
    "        oracle = None\n",
    "        for i in range(16):\n",
    "            if i == 7: INIT_FLAG = True\n",
    "            \n",
    "            if not INIT_FLAG:\n",
    "                models = fit_K_models(train_X, train_y, oracle, models, p=0.9)\n",
    "                if len(models) == 1:\n",
    "                    INIT_FLAG = True  \n",
    "            \n",
    "            if INIT_FLAG:\n",
    "                model_types = [1,4,5,6] + [1,1,6,6,6,6,6,6]\n",
    "                models = fit_initial_K_models(train_X, train_y, model_types)\n",
    "                INIT_FLAG = False\n",
    "            \n",
    "            K = len(models)\n",
    "            print(\"Iteration %d K is %d\" % (i+1, K))\n",
    "            if K == 1:\n",
    "                INIT_FLAG = True\n",
    "\n",
    "            if not INIT_FLAG:\n",
    "                X_ext, y_ext, w_ext = create_extended_dataset(train_X, train_y, models, p=0.9)\n",
    "                train_ds = OracleDataset(X_ext, y_ext, w_ext)\n",
    "                train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                oracle = create_oracle_model(train_X.shape[1], K, N).cuda()\n",
    "                train_model(oracle, train_dl, K, learning_rate, N_iter)\n",
    "            \n",
    "            \n",
    "            if not INIT_FLAG:\n",
    "                train_loss, train_r2 = compute_loss(train_X, train_y, oracle, models)\n",
    "                valid_loss, valid_r2 = compute_loss(valid_X, valid_y, oracle, models)\n",
    "                test_loss, test_r2 = compute_loss(test_X, test_y, oracle, models)\n",
    "\n",
    "\n",
    "            print(\"train loss %.3f valid loss %.3f\", train_loss, valid_loss)\n",
    "            print(\"train R^2 %.3f valid R^2 %.3f\", train_r2, valid_r2)\n",
    "            if valid_r2 >= best_valid_r2:\n",
    "                best_train_r2 = train_r2\n",
    "                best_valid_r2 = valid_r2\n",
    "                best_K = K\n",
    "                best_models = models\n",
    "                best_model_types = [m.model_type for m in models]\n",
    "                best_test_r2 = test_r2 \n",
    "        \n",
    "        results = \"dataset %s state %d K %d test ISL %.3f valid ISL %.3f model_types %s\" % (\n",
    "                dataset, state, len(best_models), best_test_r2, best_valid_r2, str(best_model_types))\n",
    "        print(results)\n",
    "        #f.write(results)\n",
    "        #f.write('\\n')\n",
    "        #f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best valid R^2 0.431 best model type 6\n",
      "Number of training points 12247, number iterations 33\n",
      "Iteration 1 K is 12\n",
      "epoch 0 loss 11.6812\n",
      "epoch 4 loss 11.4819\n",
      "epoch 8 loss 11.4686\n",
      "epoch 12 loss 11.4531\n",
      "epoch 16 loss 11.4357\n",
      "epoch 20 loss 11.4091\n",
      "epoch 24 loss 11.4242\n",
      "epoch 28 loss 11.4215\n",
      "epoch 32 loss 11.4777\n",
      "train loss %.3f valid loss %.3f 159.17840424890736 171.0000519119524\n",
      "train R^2 %.3f valid R^2 %.3f 0.1341184298920317 0.10739758770594832\n",
      "model_type 4\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 2 K is 3\n",
      "epoch 0 loss 52.7339\n",
      "epoch 4 loss 45.9298\n",
      "epoch 8 loss 45.8164\n",
      "epoch 12 loss 45.7798\n",
      "epoch 16 loss 45.7741\n",
      "epoch 20 loss 45.6569\n",
      "epoch 24 loss 45.6301\n",
      "epoch 28 loss 45.5794\n",
      "epoch 32 loss 45.5386\n",
      "train loss %.3f valid loss %.3f 128.58477640502525 189.36292023866565\n",
      "train R^2 %.3f valid R^2 %.3f 0.2106928664113198 0.23977189070327876\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 3 K is 2\n",
      "epoch 0 loss 69.5979\n",
      "epoch 4 loss 65.6209\n",
      "epoch 8 loss 65.6185\n",
      "epoch 12 loss 65.6187\n",
      "epoch 16 loss 65.6191\n",
      "epoch 20 loss 65.6193\n",
      "epoch 24 loss 65.6195\n",
      "epoch 28 loss 65.6196\n",
      "epoch 32 loss 65.6196\n",
      "train loss %.3f valid loss %.3f 130.89090243977535 146.19380257026478\n",
      "train R^2 %.3f valid R^2 %.3f 0.4790894921973603 0.407525389364469\n",
      "model_type 6\n",
      "Iteration 4 K is 12\n",
      "epoch 0 loss 11.6507\n",
      "epoch 4 loss 11.2979\n",
      "epoch 8 loss 11.2987\n",
      "epoch 12 loss 11.2532\n",
      "epoch 16 loss 11.2774\n",
      "epoch 20 loss 11.2714\n",
      "epoch 24 loss 11.2968\n",
      "epoch 28 loss 11.2766\n",
      "epoch 32 loss 11.3036\n",
      "train loss %.3f valid loss %.3f 150.50274155980136 161.0040065052198\n",
      "train R^2 %.3f valid R^2 %.3f 0.25923369101249794 0.1877899194600604\n",
      "model_type 5\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 5 K is 3\n",
      "epoch 0 loss 54.9095\n",
      "epoch 4 loss 44.8146\n",
      "epoch 8 loss 44.5663\n",
      "epoch 12 loss 44.4913\n",
      "epoch 16 loss 44.4512\n",
      "epoch 20 loss 44.3827\n",
      "epoch 24 loss 44.3143\n",
      "epoch 28 loss 44.2777\n",
      "epoch 32 loss 44.2628\n",
      "train loss %.3f valid loss %.3f 149.21267958446597 160.16930219808364\n",
      "train R^2 %.3f valid R^2 %.3f 0.24195028149554176 0.18300732598594693\n",
      "model_type 5\n",
      "model_type 1\n",
      "Iteration 6 K is 2\n",
      "epoch 0 loss 70.9832\n",
      "epoch 4 loss 67.2369\n",
      "epoch 8 loss 67.1190\n",
      "epoch 12 loss 67.1937\n",
      "epoch 16 loss 67.3003\n",
      "epoch 20 loss 67.3505\n",
      "epoch 24 loss 67.3559\n",
      "epoch 28 loss 67.2718\n",
      "epoch 32 loss 67.4548\n",
      "train loss %.3f valid loss %.3f 148.31623491962165 159.48126839832548\n",
      "train R^2 %.3f valid R^2 %.3f 0.2651744382327932 0.2002839813355679\n",
      "model_type 5\n",
      "model_type 1\n",
      "Iteration 7 K is 2\n",
      "epoch 0 loss 70.9874\n",
      "epoch 4 loss 66.6839\n",
      "epoch 8 loss 66.6937\n",
      "epoch 12 loss 66.8469\n",
      "epoch 16 loss 66.6442\n",
      "epoch 20 loss 66.8027\n",
      "epoch 24 loss 66.6842\n",
      "epoch 28 loss 66.6516\n",
      "epoch 32 loss 66.4573\n",
      "train loss %.3f valid loss %.3f 144.25140546485153 156.1627694327186\n",
      "train R^2 %.3f valid R^2 %.3f 0.26265956043770644 0.1905474197803455\n",
      "Iteration 8 K is 12\n",
      "epoch 0 loss 11.6202\n",
      "epoch 4 loss 11.4623\n",
      "epoch 8 loss 11.4098\n",
      "epoch 12 loss 11.4223\n",
      "epoch 16 loss 11.3771\n",
      "epoch 20 loss 11.4737\n",
      "epoch 24 loss 11.5050\n",
      "epoch 28 loss 11.5105\n",
      "epoch 32 loss 11.5493\n",
      "train loss %.3f valid loss %.3f 138.80979137936023 140.47619918685953\n",
      "train R^2 %.3f valid R^2 %.3f 0.4501276902471745 0.44131779707545926\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 9 K is 2\n",
      "epoch 0 loss 85.6427\n",
      "epoch 4 loss 69.4172\n",
      "epoch 8 loss 69.3950\n",
      "epoch 12 loss 69.3857\n",
      "epoch 16 loss 69.3889\n",
      "epoch 20 loss 69.3923\n",
      "epoch 24 loss 69.3944\n",
      "epoch 28 loss 69.3943\n",
      "epoch 32 loss 69.3937\n",
      "train loss %.3f valid loss %.3f 138.67599549518243 140.89336312705043\n",
      "train R^2 %.3f valid R^2 %.3f 0.44810692044338507 0.4290062985419034\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 10 K is 2\n",
      "epoch 0 loss 92.0176\n",
      "epoch 4 loss 69.5382\n",
      "epoch 8 loss 69.5594\n",
      "epoch 12 loss 69.5726\n",
      "epoch 16 loss 69.5783\n",
      "epoch 20 loss 69.5827\n",
      "epoch 24 loss 69.5844\n",
      "epoch 28 loss 69.5845\n",
      "epoch 32 loss 69.5840\n",
      "train loss %.3f valid loss %.3f 138.5650859493345 140.9634704481757\n",
      "train R^2 %.3f valid R^2 %.3f 0.4485483105382726 0.4287221770055857\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 11 K is 2\n",
      "epoch 0 loss 81.0362\n",
      "epoch 4 loss 69.2851\n",
      "epoch 8 loss 69.2729\n",
      "epoch 12 loss 69.2788\n",
      "epoch 16 loss 69.2822\n",
      "epoch 20 loss 69.2841\n",
      "epoch 24 loss 69.2847\n",
      "epoch 28 loss 69.2849\n",
      "epoch 32 loss 69.2843\n",
      "train loss %.3f valid loss %.3f 138.56896880107507 140.86256211587136\n",
      "train R^2 %.3f valid R^2 %.3f 0.44853285783503494 0.4291311247437306\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 12 K is 2\n",
      "epoch 0 loss 88.1006\n",
      "epoch 4 loss 69.5961\n",
      "epoch 8 loss 69.5773\n",
      "epoch 12 loss 69.5976\n",
      "epoch 16 loss 69.6071\n",
      "epoch 20 loss 69.6074\n",
      "epoch 24 loss 69.6033\n",
      "epoch 28 loss 69.5959\n",
      "epoch 32 loss 69.5803\n",
      "train loss %.3f valid loss %.3f 138.57513598852302 140.8623766429615\n",
      "train R^2 %.3f valid R^2 %.3f 0.4485083140914643 0.4291318764034324\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 13 K is 2\n",
      "epoch 0 loss 91.7274\n",
      "epoch 4 loss 69.5661\n",
      "epoch 8 loss 68.4739\n",
      "epoch 12 loss 67.8365\n",
      "epoch 16 loss 67.5401\n",
      "epoch 20 loss 67.5151\n",
      "epoch 24 loss 67.4401\n",
      "epoch 28 loss 67.3702\n",
      "epoch 32 loss 67.3324\n",
      "train loss %.3f valid loss %.3f 130.8699170920173 131.99527672933837\n",
      "train R^2 %.3f valid R^2 %.3f 0.47381735190028773 0.4626413572468506\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 14 K is 2\n",
      "epoch 0 loss 97.5893\n",
      "epoch 4 loss 69.1681\n",
      "epoch 8 loss 69.1925\n",
      "epoch 12 loss 69.2095\n",
      "epoch 16 loss 69.2189\n",
      "epoch 20 loss 69.2241\n",
      "epoch 24 loss 69.2261\n",
      "epoch 28 loss 69.2267\n",
      "epoch 32 loss 69.2258\n",
      "train loss %.3f valid loss %.3f 138.6781766285788 140.8342195938017\n",
      "train R^2 %.3f valid R^2 %.3f 0.44809824012042987 0.42924598751104637\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 15 K is 2\n",
      "epoch 0 loss 84.3025\n",
      "epoch 4 loss 69.3869\n",
      "epoch 8 loss 69.3653\n",
      "epoch 12 loss 69.3851\n",
      "epoch 16 loss 69.3906\n",
      "epoch 20 loss 69.3891\n",
      "epoch 24 loss 69.3878\n",
      "epoch 28 loss 69.3795\n",
      "epoch 32 loss 69.3649\n",
      "train loss %.3f valid loss %.3f 138.57967227633634 140.81691439620485\n",
      "train R^2 %.3f valid R^2 %.3f 0.4484902608888022 0.4293161196919453\n",
      "model_type 1\n",
      "model_type 6\n",
      "Iteration 16 K is 2\n",
      "epoch 0 loss 81.2345\n",
      "epoch 4 loss 67.6857\n",
      "epoch 8 loss 66.4070\n",
      "epoch 12 loss 66.3205\n",
      "epoch 16 loss 66.1823\n",
      "epoch 20 loss 66.1436\n",
      "epoch 24 loss 66.1177\n",
      "epoch 28 loss 66.1814\n",
      "epoch 32 loss 66.2505\n",
      "train loss %.3f valid loss %.3f 131.39156381329062 131.03564578561327\n",
      "train R^2 %.3f valid R^2 %.3f 0.4696492221412817 0.464446344031179\n",
      "dataset 1199_BNG_echoMonths state 1 K 2 test ISL 0.458 valid ISL 0.464 model_types [1, 6]\n"
     ]
    }
   ],
   "source": [
    "main_loop(1, [\"1199_BNG_echoMonths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
